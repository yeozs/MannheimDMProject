{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3c4e1c",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Importing Libraries](#1.-Importing-Libraries)\n",
    "2. [Importing Data](#2.-Importing-Data)  \n",
    "3. [Modelling](#3.-Modelling)  \n",
    "    3.1 [Model Training](#3.1-Model-Training) <br>\n",
    "    3.2 [Model Evaluation](#3.2-Model-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c264552",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7f7c449",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:17:21.672516Z",
     "start_time": "2024-05-17T10:17:21.651252Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3a30d40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:17:22.563243Z",
     "start_time": "2024-05-17T10:17:22.553616Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Let PyTorch use GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae22dd",
   "metadata": {},
   "source": [
    "## 2. Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "608577e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:17:24.237621Z",
     "start_time": "2024-05-17T10:17:23.921354Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/Users/antoniooliveira/MannheimWMProject/processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e95de82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:17:25.212513Z",
     "start_time": "2024-05-17T10:17:25.181991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Name</th>\n",
       "      <th>Review Date</th>\n",
       "      <th>Airline</th>\n",
       "      <th>Verified</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Type of Traveller</th>\n",
       "      <th>Month Flown</th>\n",
       "      <th>Route</th>\n",
       "      <th>Class</th>\n",
       "      <th>...</th>\n",
       "      <th>Staff Service</th>\n",
       "      <th>Food &amp; Beverages</th>\n",
       "      <th>Inflight Entertainment</th>\n",
       "      <th>Value For Money</th>\n",
       "      <th>Recommended</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Year</th>\n",
       "      <th>review_length</th>\n",
       "      <th>Reviews_1</th>\n",
       "      <th>Reviews_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Flight was amazing</td>\n",
       "      <td>Alison Soetantyo</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>True</td>\n",
       "      <td>Flight was amazing. The crew onboard this fl...</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>December 2023</td>\n",
       "      <td>Jakarta to Singapore</td>\n",
       "      <td>Business Class</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>467</td>\n",
       "      <td>Flight amazing. The crew onboard flight welcom...</td>\n",
       "      <td>flight amazing crew onboard flight welcoming g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seats on this aircraft are dreadful</td>\n",
       "      <td>Robert Watson</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>Singapore Airlines</td>\n",
       "      <td>True</td>\n",
       "      <td>Booking an emergency exit seat still meant h...</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>February 2024</td>\n",
       "      <td>Phuket to Singapore</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>2024</td>\n",
       "      <td>249</td>\n",
       "      <td>Booking emergency exit seat still meant huge d...</td>\n",
       "      <td>booking emergency exit seat still meant huge d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title              Name Review Date  \\\n",
       "0                    Flight was amazing  Alison Soetantyo  2024-03-01   \n",
       "1  seats on this aircraft are dreadful      Robert Watson  2024-02-21   \n",
       "\n",
       "              Airline Verified  \\\n",
       "0  Singapore Airlines     True   \n",
       "1  Singapore Airlines     True   \n",
       "\n",
       "                                             Reviews Type of Traveller  \\\n",
       "0    Flight was amazing. The crew onboard this fl...      Solo Leisure   \n",
       "1    Booking an emergency exit seat still meant h...      Solo Leisure   \n",
       "\n",
       "     Month Flown                 Route           Class  ...  Staff Service  \\\n",
       "0  December 2023  Jakarta to Singapore  Business Class  ...              4   \n",
       "1  February 2024   Phuket to Singapore   Economy Class  ...              3   \n",
       "\n",
       "   Food & Beverages  Inflight Entertainment  Value For Money  Recommended  \\\n",
       "0                 4                       4                4          yes   \n",
       "1                 4                       4                1           no   \n",
       "\n",
       "  Sentiment  Year  review_length  \\\n",
       "0         2  2024            467   \n",
       "1         0  2024            249   \n",
       "\n",
       "                                           Reviews_1  \\\n",
       "0  Flight amazing. The crew onboard flight welcom...   \n",
       "1  Booking emergency exit seat still meant huge d...   \n",
       "\n",
       "                                           Reviews_2  \n",
       "0  flight amazing crew onboard flight welcoming g...  \n",
       "1  booking emergency exit seat still meant huge d...  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5637a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:40:28.495971Z",
     "start_time": "2024-05-17T09:40:28.491182Z"
    }
   },
   "source": [
    "## 3. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba0d3a0",
   "metadata": {},
   "source": [
    "**Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87139918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:47:20.650717Z",
     "start_time": "2024-05-17T09:47:20.628376Z"
    }
   },
   "outputs": [],
   "source": [
    "Y = dataset[\"Sentiment\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset['Reviews_2'], Y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05738d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:47:20.664505Z",
     "start_time": "2024-05-17T09:47:20.652760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "2    3171\n",
      "0    2703\n",
      "1     605\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efae842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:40:55.878905Z",
     "start_time": "2024-05-17T09:40:55.873052Z"
    }
   },
   "source": [
    "**Reshape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2a0a4a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:47:20.671738Z",
     "start_time": "2024-05-17T09:47:20.666528Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_2d = np.array(X_train).reshape(-1, 1)\n",
    "Y_train_2d = np.array(Y_train).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a656dc0",
   "metadata": {},
   "source": [
    "**Applying Bert Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9a853a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:47:33.541152Z",
     "start_time": "2024-05-17T09:47:20.673794Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bModel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 3,)\n",
    "\n",
    "X_train_tok = []\n",
    "X_test_tok = []\n",
    "\n",
    "def encode(reviewSet, newList):\n",
    "  for review in reviewSet:\n",
    "    encodedReview = bTokenizer.encode_plus(\n",
    "      text = review,\n",
    "      add_special_tokens = True,\n",
    "      max_length=512,\n",
    "      truncation=True,\n",
    "    )\n",
    "    newList.append(encodedReview)\n",
    "  return newList\n",
    "\n",
    "X_train_tok = encode(X_train, X_train_tok)\n",
    "X_test_tok = encode(X_test, X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61a403b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:47:33.546544Z",
     "start_time": "2024-05-17T09:47:33.543149Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(X_train_tok[0])\n",
    "# print(bTokenizer.convert_ids_to_tokens(X_train_tok[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025fb15f",
   "metadata": {},
   "source": [
    "**Preparing the Dataset for Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16d9e24a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:47:33.977492Z",
     "start_time": "2024-05-17T09:47:33.549373Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "# --- Preparing the train dataset for the DataLoader ---\n",
    "\n",
    "# Convert the input sentences (tokenized) to tensors\n",
    "X_TESTER_input = [torch.tensor(nSentence['input_ids']) for nSentence in X_train_tok[:]]\n",
    "# Pad the sequences so that all tensors in the batch have the same length\n",
    "X_TESTER_input_pad = pad_sequence(X_TESTER_input, batch_first=True)[:]\n",
    "# Convert the attention masks to tensors\n",
    "X_TESTER_mask = [torch.tensor(nSentence['attention_mask']) for nSentence in X_train_tok[:]]\n",
    "# Pad the attention masks to match the input sequences\n",
    "X_TESTER_mask_pad = pad_sequence(X_TESTER_mask, batch_first=True)[:]\n",
    "\n",
    "# Convert the labels to tensors\n",
    "Y_TESTER_input = torch.tensor(list(Y_train)[:])\n",
    "# Create a TensorDataset with the padded input sequences, masks, and labels\n",
    "TESTER_dataset = TensorDataset(X_TESTER_input_pad, X_TESTER_mask_pad, Y_TESTER_input)\n",
    "\n",
    "# Create a DataLoader from the dataset, which will handle batching\n",
    "TESTER_dataLoader = DataLoader(TESTER_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- Preparing the test dataset for the DataLoader ---\n",
    "\n",
    "# Convert the input sentences (tokenized) to tensors\n",
    "X_TESTTESTER_input = [torch.tensor(nSentence['input_ids']) for nSentence in X_test_tok[:]]\n",
    "# Pad the sequences so that all tensors in the batch have the same length\n",
    "X_TESTTESTER_input_pad = pad_sequence(X_TESTTESTER_input, batch_first=True)[:]\n",
    "# Convert the attention masks to tensors\n",
    "X_TESTTESTER_mask = [torch.tensor(nSentence['attention_mask']) for nSentence in X_test_tok[:]]\n",
    "# Pad the attention masks to match the input sequences\n",
    "X_TESTTESTER_mask_pad = pad_sequence(X_TESTTESTER_mask, batch_first=True)[:]\n",
    "\n",
    "# Convert the labels to tensors\n",
    "Y_TESTTESTER_input = torch.tensor(list(Y_test)[:])\n",
    "# Create a TensorDataset with the padded input sequences, masks, and labels\n",
    "TESTTESTER_dataset = TensorDataset(X_TESTTESTER_input_pad, X_TESTTESTER_mask_pad, Y_TESTTESTER_input)\n",
    "\n",
    "# Create a DataLoader from the dataset, which will handle batching\n",
    "TESTTESTER_dataLoader = DataLoader(TESTTESTER_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5371236f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:47:33.982009Z",
     "start_time": "2024-05-17T09:47:33.979715Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(X_TESTER_mask_pad)\n",
    "# #print(X_TESTTESTER_mask_pad)\n",
    "# print(Y_TESTER_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e57f1",
   "metadata": {},
   "source": [
    "**Setting up AdmW Optimiser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78d81d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:47:34.021851Z",
     "start_time": "2024-05-17T09:47:33.984335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the AdamW optimizer with the model's parameters and a learning rate\n",
    "optimizer = AdamW(bModel.parameters(), lr=1e-6)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# Setting up the learning rate scheduler\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=len(TESTER_dataset) * EPOCHS \n",
    ")\n",
    "\n",
    "# Defining the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ecbc80",
   "metadata": {},
   "source": [
    "### 3.1 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae47b974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:15:45.198203Z",
     "start_time": "2024-05-17T09:47:34.024032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 2, 2, 0, 0, 0, 0, 1, 2, 2])\n",
      "tensor(1.0854, grad_fn=<NllLossBackward0>)\n",
      "0.4\n",
      "pass done1\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([1, 0, 0, 0, 0, 2, 0, 2, 0, 2])\n",
      "tensor(1.0815, grad_fn=<NllLossBackward0>)\n",
      "0.3\n",
      "pass done2\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 0, 2, 2, 0, 0, 0, 0, 0, 2])\n",
      "tensor(1.0401, grad_fn=<NllLossBackward0>)\n",
      "0.4\n",
      "pass done3\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([1, 2, 2, 0, 0, 2, 0, 2, 0, 2])\n",
      "tensor(1.0399, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done4\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 1, 0, 0, 2, 0, 2, 1, 2, 2])\n",
      "tensor(1.0014, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done5\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 2, 2, 2, 0, 2, 0, 2, 0, 0])\n",
      "tensor(0.9983, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done6\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 0, 2, 0, 2, 0, 2, 0, 2, 1])\n",
      "tensor(1.0565, grad_fn=<NllLossBackward0>)\n",
      "0.4\n",
      "pass done7\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 2, 0, 2, 0, 0, 2, 0, 0, 2])\n",
      "tensor(1.0427, grad_fn=<NllLossBackward0>)\n",
      "0.4\n",
      "pass done8\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 0, 0, 0, 0, 2, 0, 0, 2, 0])\n",
      "tensor(1.0225, grad_fn=<NllLossBackward0>)\n",
      "0.3\n",
      "pass done9\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 2, 0, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor(0.8847, grad_fn=<NllLossBackward0>)\n",
      "0.8\n",
      "pass done10\n",
      "tensor([2, 0, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 0, 2, 2, 2, 2, 2, 0, 0, 2])\n",
      "tensor(0.8531, grad_fn=<NllLossBackward0>)\n",
      "0.8\n",
      "pass done11\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 0, 0, 0, 2, 0, 2, 2, 0])\n",
      "tensor(0.9994, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done12\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 2, 0, 2, 2, 2, 0, 2, 2, 2])\n",
      "tensor(0.8516, grad_fn=<NllLossBackward0>)\n",
      "0.7\n",
      "pass done13\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 2, 0, 2, 0, 0, 2, 1, 2, 2])\n",
      "tensor(0.9698, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done14\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 0, 2, 0, 2, 2])\n",
      "tensor(0.8829, grad_fn=<NllLossBackward0>)\n",
      "0.8\n",
      "pass done15\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 2, 0, 2, 2, 0, 2, 2, 0, 2])\n",
      "tensor(0.9128, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done16\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 0, 1, 2, 1, 2, 0, 2, 1, 2])\n",
      "tensor(1.0742, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done17\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 0, 0, 2, 2, 2, 0, 0, 2, 0])\n",
      "tensor(0.9818, grad_fn=<NllLossBackward0>)\n",
      "0.4\n",
      "pass done18\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0])\n",
      "tensor([0, 2, 2, 1, 0, 0, 2, 0, 0, 0])\n",
      "tensor(0.9994, grad_fn=<NllLossBackward0>)\n",
      "0.4\n",
      "pass done19\n",
      "tensor([2, 2, 2, 2, 2, 2, 0, 0, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 0, 2, 2, 0])\n",
      "tensor(0.8114, grad_fn=<NllLossBackward0>)\n",
      "0.8\n",
      "pass done20\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 0, 2])\n",
      "tensor([2, 1, 2, 0, 2, 2, 2, 0, 0, 2])\n",
      "tensor(0.9473, grad_fn=<NllLossBackward0>)\n",
      "0.7\n",
      "pass done21\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 0, 2, 0, 2, 2])\n",
      "tensor(0.8907, grad_fn=<NllLossBackward0>)\n",
      "0.8\n",
      "pass done22\n",
      "tensor([2, 2, 0, 0, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 0, 0, 0, 2, 0, 1, 2, 2, 1])\n",
      "tensor(1.0384, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done23\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 0, 2, 2])\n",
      "tensor([0, 2, 2, 2, 2, 1, 2, 0, 2, 1])\n",
      "tensor(0.9744, grad_fn=<NllLossBackward0>)\n",
      "0.7\n",
      "pass done24\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 0, 0, 2, 2, 2, 2, 0, 2, 0])\n",
      "tensor(0.8753, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done25\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 0, 0, 0])\n",
      "tensor([0, 0, 0, 2, 2, 0, 2, 0, 2, 0])\n",
      "tensor(0.8946, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done26\n",
      "tensor([0, 2, 2, 2, 2, 2, 2, 2, 2, 0])\n",
      "tensor([1, 0, 2, 2, 2, 1, 0, 2, 2, 2])\n",
      "tensor(1.0248, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done27\n",
      "tensor([0, 2, 2, 0, 2, 0, 2, 2, 2, 2])\n",
      "tensor([2, 2, 0, 0, 0, 0, 2, 0, 1, 0])\n",
      "tensor(0.9997, grad_fn=<NllLossBackward0>)\n",
      "0.4\n",
      "pass done28\n",
      "tensor([2, 0, 2, 0, 0, 2, 2, 2, 2, 2])\n",
      "tensor([0, 2, 0, 0, 1, 2, 0, 0, 0, 1])\n",
      "tensor(1.0882, grad_fn=<NllLossBackward0>)\n",
      "0.2\n",
      "pass done29\n",
      "tensor([2, 2, 2, 2, 0, 2, 0, 2, 2, 2])\n",
      "tensor([0, 0, 2, 2, 1, 0, 1, 0, 0, 0])\n",
      "tensor(1.1258, grad_fn=<NllLossBackward0>)\n",
      "0.2\n",
      "pass done30\n",
      "tensor([2, 2, 2, 0, 2, 2, 0, 2, 2, 0])\n",
      "tensor([2, 0, 0, 2, 2, 2, 0, 2, 2, 2])\n",
      "tensor(0.8539, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done31\n",
      "tensor([2, 2, 2, 0, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 0, 2, 0, 0, 1, 2, 2, 0, 0])\n",
      "tensor(1.0241, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done32\n",
      "tensor([2, 2, 0, 2, 2, 2, 2, 0, 0, 2])\n",
      "tensor([2, 2, 0, 2, 1, 0, 2, 0, 2, 0])\n",
      "tensor(0.9253, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done33\n",
      "tensor([2, 2, 0, 2, 2, 2, 2, 0, 2, 2])\n",
      "tensor([0, 0, 2, 2, 2, 2, 0, 2, 0, 0])\n",
      "tensor(0.9777, grad_fn=<NllLossBackward0>)\n",
      "0.3\n",
      "pass done34\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0])\n",
      "tensor([1, 0, 2, 2, 2, 2, 0, 0, 2, 2])\n",
      "tensor(0.9608, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done35\n",
      "tensor([0, 0, 2, 2, 2, 2, 2, 0, 2, 2])\n",
      "tensor([1, 2, 0, 2, 2, 1, 1, 0, 2, 2])\n",
      "tensor(1.0559, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done36\n",
      "tensor([0, 2, 0, 2, 0, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 0, 2, 0, 2, 0, 2, 0, 2])\n",
      "tensor(0.8599, grad_fn=<NllLossBackward0>)\n",
      "0.7\n",
      "pass done37\n",
      "tensor([2, 2, 2, 2, 0, 2, 0, 0, 0, 2])\n",
      "tensor([2, 2, 0, 2, 2, 1, 0, 1, 2, 0])\n",
      "tensor(1.0209, grad_fn=<NllLossBackward0>)\n",
      "0.4\n",
      "pass done38\n",
      "tensor([2, 2, 0, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 0, 1, 0, 0, 0, 2, 2, 0])\n",
      "tensor(1.0286, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done39\n",
      "tensor([2, 2, 0, 0, 0, 2, 2, 0, 2, 2])\n",
      "tensor([0, 2, 0, 2, 0, 0, 0, 0, 0, 2])\n",
      "tensor(0.9455, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done40\n",
      "tensor([0, 0, 0, 2, 2, 0, 2, 0, 2, 2])\n",
      "tensor([0, 2, 2, 0, 0, 2, 1, 2, 2, 0])\n",
      "tensor(1.0209, grad_fn=<NllLossBackward0>)\n",
      "0.2\n",
      "pass done41\n",
      "tensor([0, 2, 2, 2, 0, 0, 2, 2, 0, 2])\n",
      "tensor([0, 2, 2, 0, 0, 2, 0, 2, 0, 0])\n",
      "tensor(0.8778, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done42\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 2, 0, 0, 2, 2, 2, 0, 1, 0])\n",
      "tensor(0.9745, grad_fn=<NllLossBackward0>)\n",
      "0.4\n",
      "pass done43\n",
      "tensor([0, 2, 2, 2, 2, 2, 0, 2, 2, 0])\n",
      "tensor([0, 2, 2, 2, 1, 0, 2, 1, 2, 1])\n",
      "tensor(1.0801, grad_fn=<NllLossBackward0>)\n",
      "0.5\n",
      "pass done44\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0])\n",
      "tensor([2, 2, 2, 0, 2, 0, 0, 0, 2, 0])\n",
      "tensor(0.8916, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done45\n",
      "tensor([0, 2, 2, 2, 2, 2, 2, 2, 2, 0])\n",
      "tensor([0, 0, 2, 2, 2, 2, 0, 2, 0, 0])\n",
      "tensor(0.9059, grad_fn=<NllLossBackward0>)\n",
      "0.7\n",
      "pass done46\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 0, 0, 2])\n",
      "tensor([2, 2, 0, 2, 1, 2, 0, 0, 2, 2])\n",
      "tensor(0.9235, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done47\n",
      "tensor([2, 2, 2, 2, 2, 0, 0, 2, 2, 2])\n",
      "tensor([2, 0, 2, 0, 1, 2, 2, 0, 1, 0])\n",
      "tensor(1.0785, grad_fn=<NllLossBackward0>)\n",
      "0.2\n",
      "pass done48\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 0, 2, 2, 1, 2, 0])\n",
      "tensor(0.9771, grad_fn=<NllLossBackward0>)\n",
      "0.7\n",
      "pass done49\n",
      "tensor([2, 2, 2, 2, 0, 2, 2, 2, 2, 2])\n",
      "tensor([0, 1, 2, 1, 2, 0, 2, 2, 0, 2])\n",
      "tensor(1.0395, grad_fn=<NllLossBackward0>)\n",
      "0.4\n",
      "pass done50\n",
      "tensor([0, 2, 2, 0, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 2, 0, 0, 2, 2, 2, 0, 0, 0])\n",
      "tensor(0.8899, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done51\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([2, 2, 0, 0, 2, 2, 0, 1, 2, 2])\n",
      "tensor(0.9249, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done52\n",
      "tensor([2, 2, 2, 2, 2, 2, 0, 2, 2, 2])\n",
      "tensor([2, 2, 0, 2, 2, 2, 0, 2, 0, 2])\n",
      "tensor(0.8275, grad_fn=<NllLossBackward0>)\n",
      "0.8\n",
      "pass done53\n",
      "tensor([2, 2, 0, 0, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 0, 0, 1, 2, 2, 2, 2, 2, 2])\n",
      "tensor(0.9665, grad_fn=<NllLossBackward0>)\n",
      "0.7\n",
      "pass done54\n",
      "tensor([0, 2, 0, 2, 0, 2, 2, 0, 2, 0])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 0])\n",
      "tensor(0.8828, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done55\n",
      "tensor([2, 2, 2, 0, 0, 2, 2, 2, 0, 2])\n",
      "tensor([2, 2, 2, 2, 2, 1, 1, 2, 0, 2])\n",
      "tensor(0.9744, grad_fn=<NllLossBackward0>)\n",
      "0.6\n",
      "pass done56\n",
      "tensor([2, 2, 2, 2, 2, 0, 2, 2, 2, 2])\n",
      "tensor([2, 2, 0, 2, 2, 1, 2, 2, 0, 2])\n",
      "tensor(0.9259, grad_fn=<NllLossBackward0>)\n",
      "0.7\n",
      "pass done57\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "tensor([0, 0, 0, 2, 0, 0, 2, 0, 1, 0])\n",
      "tensor(0.9975, grad_fn=<NllLossBackward0>)\n",
      "0.2\n",
      "pass done58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = 0\n",
    "counter = 0\n",
    "accuracyTrain_list = []\n",
    "lossTrain_list = []\n",
    "avg_accuracyTrain_list = []\n",
    "prediction_list = []\n",
    "actual_list = []\n",
    "\n",
    "bModel.to(device)  \n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # Inform that model training is beginning\n",
    "    bModel.train()  \n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for item in TESTER_dataLoader:\n",
    "        # Restart optimizer values\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = item[0].to(device)  \n",
    "        attention_masks = item[1].to(device)  \n",
    "        labels = item[2].to(device)  \n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = bModel(input_ids=input_ids,\n",
    "                         attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        predictions = torch.argmax(outputs.logits, dim=1) \n",
    "\n",
    "        # Print the predictions, labels, and loss for debugging\n",
    "        print(predictions)\n",
    "        print(labels)\n",
    "        print(loss)\n",
    "        print((predictions == labels).sum().item() / predictions.size(0))\n",
    "\n",
    "        # Calculate accuracy for the current batch\n",
    "        accuracyTrain_list.append((predictions == labels).sum().item() / predictions.size(0))\n",
    "        prediction_list.extend(predictions.tolist())  \n",
    "        actual_list.extend(labels.tolist())  \n",
    "\n",
    "        # Accumulate total loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += (predictions == labels).sum().item() / predictions.size(0)\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  \n",
    "        counter += 1\n",
    "        print(\"pass done\" + str(counter)) \n",
    "\n",
    "    # Calculate and store average accuracy for the epoch\n",
    "    average_accuracy_in_epoch = total_accuracy / len(TESTER_dataLoader)\n",
    "    avg_accuracyTrain_list.append(average_accuracy_in_epoch)\n",
    "\n",
    "    # Print average train loss for the epoch\n",
    "    print(\"Average Train loss is: \" + str(total_loss / len(TESTER_dataLoader)))\n",
    "\n",
    "    # Generate and plot confusion matrix\n",
    "    cm = confusion_matrix(actual_list, prediction_list)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of Sentiment (Count)')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm / np.sum(cm), annot=True, cmap='Blues')  \n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of Sentiment (Percentage)')\n",
    "    plt.show()\n",
    "\n",
    "    # Reset lists for the next epoch\n",
    "    prediction_list = []\n",
    "    actual_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ebea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T09:38:33.794618Z",
     "start_time": "2024-05-17T09:38:33.792064Z"
    }
   },
   "source": [
    "### 3.2 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdfa3c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:15:45.204332Z",
     "start_time": "2024-05-17T10:15:45.204312Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "bModel.eval()\n",
    "\n",
    "# Initialize variables to track evaluation metrics\n",
    "total_correct = 0\n",
    "total_data_counter = 0\n",
    "prediction_list = []\n",
    "actual_list = []\n",
    "accuracyTest_list = []\n",
    "\n",
    "# Disable gradient calculation during evaluation\n",
    "with torch.no_grad():\n",
    "    for item in TESTTESTER_dataLoader:\n",
    "        # Move inputs, attention masks, and labels to the device\n",
    "        input_ids = item[0].to(device)\n",
    "        attention_masks = item[1].to(device)\n",
    "        labels = item[2].to(device)\n",
    "\n",
    "        # Perform forward pass through the model to get predictions\n",
    "        outputs = bModel(input_ids=input_ids,\n",
    "                         attention_mask=attention_masks)\n",
    "        \n",
    "        # Get predicted labels by selecting the index of the maximum logit value\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        # Update total correct predictions and total data count\n",
    "        total_correct += (predictions == labels).sum().item()\n",
    "        total_data_counter += labels.size(dim=0)\n",
    "\n",
    "        # Store predictions and actual labels \n",
    "        prediction_list.extend(predictions.tolist())\n",
    "        actual_list.extend(labels.tolist())\n",
    "\n",
    "        # Calculate accuracy for the current batch and store it\n",
    "        accuracyTest_list.append((predictions == labels).sum().item() / predictions.size(0))\n",
    "\n",
    "    # Calculate F1 score using sklearn's f1_score function\n",
    "    f1 = f1_score(actual_list, prediction_list, average=\"weighted\")\n",
    "\n",
    "    # Print accuracy, F1 score, and confusion matrix\n",
    "    print(\"Accuracy is: \" + str(total_correct / total_data_counter))\n",
    "    print(\"F1 is: \" + str(f1))\n",
    "\n",
    "    # Generate and plot confusion matrix\n",
    "    cm = confusion_matrix(actual_list, prediction_list)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of Sentiment (Count)')\n",
    "    plt.show()   \n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm/np.sum(cm), annot=True, cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of Sentiment (Percentage)')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ec3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819820b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T10:15:45.206111Z",
     "start_time": "2024-05-17T10:15:45.206093Z"
    }
   },
   "outputs": [],
   "source": [
    "conf_mat = np.array([[660, 24, 24],\n",
    "                     [70, 31, 60],\n",
    "                     [31, 29, 690]])\n",
    "\n",
    "#print(np.sum(conf_mat, axis=1))\n",
    "\n",
    "precision = np.diag(conf_mat) / np.sum(conf_mat, axis=0)\n",
    "recall = np.diag(conf_mat) / np.sum(conf_mat, axis=1)\n",
    "f1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19fede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuation\n",
    "def remove_punc(review):\n",
    "    ascii_to_translate = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    review = review.translate(ascii_to_translate)\n",
    "    return review\n",
    "\n",
    "#print(reviews_list_deemojize[4708])\n",
    "reviews_list_noPunc = [remove_punc(review) for review in reviews_list_deemojize]\n",
    "#print(reviews_list_noPunc[4708])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b5dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since emoji is present only in review 4708, de-emojize review 4708.\n",
    "reviews_list_deemojize = reviews_list.copy()\n",
    "#reviews_list_deemojize[4708] = emoji.demojize(reviews_list_deemojize[4708], language='en')\n",
    "#print(reviews_list[4708])\n",
    "#print(reviews_list_deemojize[4708])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297db0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for emojis\n",
    "def contain_emoji(review):\n",
    "    emoList = emoji.emoji_list(review)\n",
    "\n",
    "    if emoList:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "emoji_check = [contain_emoji(review) for review in reviews_list]\n",
    "\n",
    "for i in range(len(emoji_check)):\n",
    "    if emoji_check[i] == True:\n",
    "        print(\"This is Review: \" + str(i))\n",
    "        print(reviews_list[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
