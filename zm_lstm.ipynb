{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and constants\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import matutils\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import datetime\n",
    "\n",
    "from lstm_preprocessing import lstm_preprocessing\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42) \n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "MAX_SEQ_LEN = 200\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_csv(os.getcwd() + '\\\\airlines_reviews.csv')\n",
    "raw_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "tokenized_dataset = lstm_preprocessing(dataset=raw_dataset.sample(2000, ignore_index=True)) ##TODO: Remove sampling\n",
    "tokenized_reviews = tokenized_dataset['Tokenized_Reviews']\n",
    "tokenized_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding / Vectorization using TF-IDF (each review = 1 document)\n",
    "dct = Dictionary(tokenized_reviews)  # fit dictionary\n",
    "corpus = [dct.doc2bow(line) for line in tokenized_reviews]\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "print('Number of features: {}'.format(len(dct)))\n",
    "\n",
    "\n",
    "## Inspect some TF-IDF scores of first review, sorted by the similarity score\n",
    "print('Text 0:\\n{}\\n'.format(tokenized_reviews[0]))\n",
    "for w, s in sorted(tfidf_model[corpus[0]], reverse=True, key=lambda x: x[1]):\n",
    "    print('{}:{}'.format(dct[w], s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize all reviews with TF-IDF\n",
    "tfidf_vectorization_csr = matutils.corpus2csc(tfidf_model[corpus], num_terms=len(dct))\n",
    "tfidf_reviews = tfidf_vectorization_csr.T.toarray()\n",
    "print(f'TF-IDF matrix has shape: {tfidf_reviews.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding / Vectorization using GloVe\n",
    "    ## Note: LSTM processes the sentence sequentially, hence vectorization should be done word-by-word\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "\n",
    "tokenized_embedded_reviews = []\n",
    "unidentified_tokens = [] ## Tokens not in GloVe model\n",
    "\n",
    "\n",
    "for review in tokenized_reviews:\n",
    "    curr_embedded_review = []\n",
    "    \n",
    "    for token in review:\n",
    "        if token in glove_model:\n",
    "            curr_embedded_review.append(glove_model[token])\n",
    "        else:\n",
    "            unidentified_tokens.append(token)\n",
    "\n",
    "    tokenized_embedded_reviews.append(curr_embedded_review)\n",
    "\n",
    "\n",
    "print(f'Sample of embedded reviews:')\n",
    "print(f'Text: {tokenized_reviews[0]}')\n",
    "print(f'Vector: {tokenized_embedded_reviews[0]}')\n",
    "print(f'{len(unidentified_tokens)} total tokens not in GloVe model: \\n{unidentified_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset class\n",
    "    ## Uses preprocessing method above\n",
    "    ## __getitem__ returns (preprocessed) text and its corresponding label\n",
    "\n",
    "##TODO: Augment such that it can return both text and vector forms of reviews\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ret_review = self.reviews[idx]\n",
    "        ret_label = self.labels[idx]\n",
    "        \n",
    "        return ret_review, ret_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Dataset wrapper\n",
    "reviews_dataset = ReviewsDataset(reviews=tokenized_embedded_reviews, labels=tokenized_dataset['Sentiment'])\n",
    "print(f'First review: {reviews_dataset[0][0]}, \\nCorresponding label: {reviews_dataset[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split dataset\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "    reviews_dataset, [0.8, 0.1, 0.1], generator=torch.Generator()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create DataLoaders\n",
    "\n",
    "## Collate function to pad or trim reviews to same number of tokens\n",
    "def review_collate_fn(raw_batch):\n",
    "    ## Input: Collection of (review, label) tuples from ReviewDataset\n",
    "\n",
    "    padded_reviews = []\n",
    "    labels = []\n",
    "    pad_tensor = torch.zeros(len(raw_batch[0][0][0])) ## Pad with zero tensor of size equal to word embeddings\n",
    "\n",
    "    for (review, label) in raw_batch:\n",
    "        padded_review = review\n",
    "        if len(review) < MAX_SEQ_LEN:\n",
    "            padded_review = padded_review + [pad_tensor for i in range(MAX_SEQ_LEN - len(review))]\n",
    "        elif len(review) > MAX_SEQ_LEN:\n",
    "            padded_review = padded_review[:MAX_SEQ_LEN]\n",
    "        padded_reviews.append(padded_review)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # print(torch.Tensor(padded_reviews).shape)\n",
    "\n",
    "    ## Returns: a tuple (review tensor, label tensor) of sizes batch_size*MAX_SEQ_LEN and batch_size, respectively.\n",
    "    return torch.Tensor(padded_reviews), torch.Tensor(labels)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=review_collate_fn)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=review_collate_fn)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=review_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check batches\n",
    "example_features, example_label = next(iter(train_loader))\n",
    "print(f'Sample feature: \\n{example_features}, \\nFeature size: {example_features.shape}')\n",
    "print(f'Sample label: \\n{example_label}')\n",
    "\n",
    "## Assert that feature size is (batch_size, sequence_length ie review_length, feature_size ie word_vec_size)\n",
    "assert example_features.shape == torch.Size([10, 200, 50]), 'Batch provided by DataLoader is of wrong size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM model\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_lstm_layers=1, cell_dropout=0.0):\n",
    "        ## vocab_size = no. of unique words in reviews\n",
    "        ## embedding_dim = size of embeddings / vectors\n",
    "        ## hidden_dim = dimension of LSTM output\n",
    "        ## num_lstm_layers = no. of LSTM layers\n",
    "        ## cell_dropout = dropout applied between LSTM layers\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        ## Model layers\n",
    "            ## Embedding layer TODO: Should this be implemented?\n",
    "            ## LSTM (for thought vector)\n",
    "            ## Linear layer (for logit score)\n",
    "            ## Activation (for P of +ve sentiment)\n",
    "\n",
    "        self.model = nn.ModuleDict({\n",
    "            'lstm': nn.LSTM(\n",
    "                input_size=embedding_dim, \n",
    "                hidden_size=self.hidden_dim, \n",
    "                num_layers=self.num_lstm_layers, \n",
    "                batch_first=True, \n",
    "                dropout=cell_dropout),\n",
    "            'linear1': nn.Linear(\n",
    "                in_features=self.hidden_dim, \n",
    "                out_features=3 ## 3 units for predicting 3 sentiments\n",
    "            ),\n",
    "            'sigmoid': nn.Sigmoid()\n",
    "        })\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## Input is a (batch_size, sequence_length, feature_size) tensor\n",
    "        ##TODO: Implement forward pass, with cell and hidden states\n",
    "\n",
    "        ## LSTM outputs\n",
    "            ## h_t = Tensor of shape (batch_size, sequence_length, direction*hidden_size) representing hidden state at each t\n",
    "            ## h_n = Hidden state at last time step\n",
    "            ## c_n = Cell state at last time step\n",
    "        h_t, (h_n, c_n) = self.model['lstm'](x)\n",
    "        # print(f'LSTM hidden states: {h_t.shape}')\n",
    "        # print(f'LSTM final state: {h_n.shape}')\n",
    "\n",
    "        output = self.model['linear1'](h_n[-1])\n",
    "        # print(f'Linear output: {output.shape}')\n",
    "\n",
    "        output = self.model['sigmoid'](output)\n",
    "        # print(f'Sigmoid output: {output.shape}')\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    ## Initialize initial cell and hidden states\n",
    "    def init_hidden(self, batch_size):\n",
    "        ##TODO: Return tuple of two num_layers * batch_size * hidden_dim tensors\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize an LSTM model\n",
    "    ## Hyperparameters\n",
    "embedding_dim = 50\n",
    "hidden_dim = 64\n",
    "num_lstm_layers = 1\n",
    "cell_dropout = 0.1\n",
    "\n",
    "model = SimpleLSTM(embedding_dim, hidden_dim, num_lstm_layers, cell_dropout)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test forward pass\n",
    "example_output = model(example_features)\n",
    "print(f'Sample output: \\n{example_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation function\n",
    "def evaluation(model:nn.Module, data_loader:DataLoader, loss_fn=nn.CrossEntropyLoss()):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        total_correct = 0\n",
    "        total_loss = 0\n",
    "        all_pred = []\n",
    "        all_labels = []\n",
    "\n",
    "        ## Process batch by batch\n",
    "        for reviews, labels in data_loader:\n",
    "            reviews, labels = reviews.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            ## Forward pass\n",
    "            pred = model(reviews)\n",
    "            pred_class = torch.argmax(pred, dim=1) ## Get indices of largest value in prediction tensor, ie predicted class\n",
    "\n",
    "            ## Calculate loss\n",
    "            loss = loss_fn(pred, labels.long())\n",
    "            total_loss += abs(loss)\n",
    "\n",
    "            all_pred.extend(pred_class.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "        \n",
    "        ## Calculate metrics\n",
    "        correct_preds = sum([1 for pred, label in zip(all_pred, all_labels) if pred == label])\n",
    "        f1 = f1_score(all_labels, all_pred, average='micro')\n",
    "\n",
    "    return total_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test evaluation function\n",
    "example_loss, example_f1 = evaluation(model, val_loader)\n",
    "print(f'Loss: {example_loss}')\n",
    "print(f'F1: {example_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training function / Optimization loop\n",
    "def train_model(\n",
    "    model:nn.Module, \n",
    "    train_loader:DataLoader, \n",
    "    val_loader:DataLoader, \n",
    "    lr=0.01, \n",
    "    epochs=3, \n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "):\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    ## Initialize optimizer for params that require gradients\n",
    "    optimizer = torch.optim.Adam([param for param in model.parameters() if param.requires_grad], lr=lr)\n",
    "\n",
    "    ## Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        epoch_all_pred = []\n",
    "        epoch_all_labels = []\n",
    "\n",
    "        ## Iterate through batches\n",
    "        for reviews, labels in train_loader:\n",
    "            model.train()\n",
    "            reviews, labels = reviews.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            ## Forward pass\n",
    "            pred = model(reviews)\n",
    "            pred_class = torch.argmax(pred, dim=1) ## Get indices of largest value in prediction tensor, ie predicted class\n",
    "\n",
    "            ## Calculate loss\n",
    "            loss = loss_fn(pred, labels.long())\n",
    "            train_loss += abs(loss)\n",
    "\n",
    "            ## Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ## Tabulate epoch outputs\n",
    "            epoch_all_pred.extend(pred_class.numpy())\n",
    "            epoch_all_labels.extend(labels.numpy())\n",
    "\n",
    "        ## Epoch statistics\n",
    "        train_f1 = f1_score(epoch_all_labels, epoch_all_pred, average='micro')\n",
    "        if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "            print(f'======== Epoch {epoch + 1} ========')\n",
    "            print(f'Training loss: {train_loss}')\n",
    "            print(f'Training F1: {train_f1}')\n",
    "\n",
    "            ## Validation\n",
    "            val_loss, val_f1 = evaluation(model, val_loader, loss_fn=loss_fn)\n",
    "            print(f'Validation loss: {val_loss}')\n",
    "            print(f'Validation F1: {val_f1}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing training loop\n",
    "embedding_dim = 50\n",
    "hidden_dim = 64\n",
    "num_lstm_layers = 2\n",
    "cell_dropout = 0.1\n",
    "\n",
    "model = SimpleLSTM(embedding_dim, hidden_dim, num_lstm_layers, cell_dropout)\n",
    "\n",
    "trained_model = train_model(model, train_loader, val_loader, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "curr_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "file_name = f\"model_{curr_datetime}.pt\"\n",
    "torch.save(model.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning\n",
    "    ## Embedding (size, method)\n",
    "    ## Hidden dimension\n",
    "    ## LSTM layers\n",
    "    ## Bidirectional\n",
    "    ## Dropout probability\n",
    "    ## Learning rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_mining_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
