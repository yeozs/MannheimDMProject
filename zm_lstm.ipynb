{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and constants\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import matutils\n",
    "import gensim.downloader as api\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from lstm_preprocessing import lstm_preprocessing\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42) \n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "MAX_SEQ_LEN = 200\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_csv(os.getcwd() + '\\\\airlines_reviews.csv')\n",
    "raw_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "tokenized_dataset = lstm_preprocessing(dataset=raw_dataset.sample(1000, ignore_index=True)) ##TODO: Remove sampling\n",
    "tokenized_reviews = tokenized_dataset['Tokenized_Reviews']\n",
    "tokenized_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding / Vectorization using TF-IDF (each review = 1 document)\n",
    "dct = Dictionary(tokenized_reviews)  # fit dictionary\n",
    "corpus = [dct.doc2bow(line) for line in tokenized_reviews]\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "print('Number of features: {}'.format(len(dct)))\n",
    "\n",
    "\n",
    "## Inspect some TF-IDF scores of first review, sorted by the similarity score\n",
    "print('Text 0:\\n{}\\n'.format(tokenized_reviews[0]))\n",
    "for w, s in sorted(tfidf_model[corpus[0]], reverse=True, key=lambda x: x[1]):\n",
    "    print('{}:{}'.format(dct[w], s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize all reviews with TF-IDF\n",
    "tfidf_vectorization_csr = matutils.corpus2csc(tfidf_model[corpus], num_terms=len(dct))\n",
    "tfidf_reviews = tfidf_vectorization_csr.T.toarray()\n",
    "print(f'TF-IDF matrix has shape: {tfidf_reviews.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding / Vectorization using GloVe\n",
    "    ## Note: LSTM processes the sentence sequentially, hence vectorization should be done word-by-word\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "\n",
    "tokenized_embedded_reviews = []\n",
    "unidentified_tokens = [] ## Tokens not in GloVe model\n",
    "\n",
    "\n",
    "for review in tokenized_reviews:\n",
    "    curr_embedded_review = []\n",
    "    \n",
    "    for token in review:\n",
    "        if token in glove_model:\n",
    "            curr_embedded_review.append(glove_model[token])\n",
    "        else:\n",
    "            unidentified_tokens.append(token)\n",
    "\n",
    "    tokenized_embedded_reviews.append(curr_embedded_review)\n",
    "\n",
    "\n",
    "print(f'Sample of embedded reviews:')\n",
    "print(f'Text: {tokenized_reviews[0]}')\n",
    "print(f'Vector: {tokenized_embedded_reviews[0]}')\n",
    "print(f'{len(unidentified_tokens)} total tokens not in GloVe model: \\n{unidentified_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset class\n",
    "    ## Uses preprocessing method above\n",
    "    ## __getitem__ returns (preprocessed) text and its corresponding label\n",
    "\n",
    "##TODO: Augment such that it can return both text and vector forms of reviews\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ret_review = self.reviews[idx]\n",
    "        ret_label = self.labels[idx]\n",
    "        \n",
    "        return ret_review, ret_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Dataset wrapper\n",
    "reviews_dataset = ReviewsDataset(reviews=tokenized_embedded_reviews, labels=tokenized_dataset['Sentiment'])\n",
    "print(f'First review: {reviews_dataset[0][0]}, \\nCorresponding label: {reviews_dataset[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split dataset\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "    reviews_dataset, [0.8, 0.1, 0.1], generator=torch.Generator()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create DataLoaders\n",
    "\n",
    "## Collate function to pad or trim reviews to same number of tokens\n",
    "def review_collate_fn(raw_tokens):\n",
    "    ##TODO: Implement function and add as argument for DataLoaders\n",
    "    pass\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check batches\n",
    "example_features, example_label = next(iter(train_loader))\n",
    "print(f'Sample feature: \\n{example_features}, \\nFeature size: {example_features.shape}')\n",
    "print(f'Sample label: \\n{example_label}')\n",
    "\n",
    "##TODO: Assert that feature size is (batch_size, sequence_length ie review_length, feature_size ie word_vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM model\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_lstm_layers=1, cell_dropout=0.0):\n",
    "        ## vocab_size = no. of unique words in reviews\n",
    "        ## embedding_dim = size of embeddings / vectors\n",
    "        ## hidden_dim = dimension of LSTM output\n",
    "        ## num_lstm_layers = no. of LSTM layers\n",
    "        ## cell_dropout = dropout applied between LSTM layers\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        ## Model layers\n",
    "            ## Embedding layer TODO: Should this be implemented?\n",
    "            ## LSTM (for thought vector)\n",
    "            ## Linear layer (for logit score)\n",
    "            ## Activation (for P of +ve sentiment)\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('lstm', nn.LSTM(embedding_dim, self.hidden_dim, self.num_lstm_layers, batch_first=True, dropout=cell_dropout)),\n",
    "            ('fcl1', nn.Linear(self.hidden_dim, 1)),\n",
    "            ('sigmoid1', nn.Sigmoid())\n",
    "        ]))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## Input is a (batch_size, sequence_length, feature_size) tensor\n",
    "        ##TODO: Implement forward pass, with cell and hidden states\n",
    "        pass\n",
    "\n",
    "\n",
    "    ## Initialize initial cell and hidden states\n",
    "    def init_hidden(self, batch_size):\n",
    "        ##TODO: Return tuple of two num_layers * batch_size * hidden_dim tensors\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize an LSTM model\n",
    "    ## Hyperparameters\n",
    "vocab_size = len(dct)\n",
    "embedding_dim = len(dct)\n",
    "hidden_dim = 32\n",
    "num_lstm_layers = 2\n",
    "cell_dropout = 0.1\n",
    "\n",
    "model = SimpleLSTM(vocab_size, embedding_dim, hidden_dim, num_lstm_layers, cell_dropout)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training function / Optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning\n",
    "    ## Embedding (size, method)\n",
    "    ## Hidden dimension\n",
    "    ## LSTM layers\n",
    "    ## Bidirectional\n",
    "    ## Dropout probability\n",
    "    ## Learning rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_mining_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
